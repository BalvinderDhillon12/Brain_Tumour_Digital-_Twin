import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import rcParams
from matplotlib.patches import Rectangle
from collections import Counter
from scipy import stats
import torch

# Configure matplotlib for publication quality
plt.rcParams.update({
    'font.family': 'serif',
    'font.size': 11,
    'axes.labelsize': 12,
    'axes.titlesize': 13,
    'xtick.labelsize': 11,
    'ytick.labelsize': 11,
    'legend.fontsize': 10,
    'figure.titlesize': 14,
    'figure.dpi': 150,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.15,
    'axes.linewidth': 1.2,
    'xtick.major.width': 1.2,
    'ytick.major.width': 1.2,
    'xtick.major.size': 5,
    'ytick.major.size': 5,
})

# ============================================================
# POLICY DEFINITIONS
# ============================================================

def random_policy(state):
    """Random baseline: uniformly samples actions."""
    return np.random.randint(0, 4)

def heuristic_policy(state, volume_index=4):
    """Rule-based heuristic: graduated response."""
    volume = state[volume_index]
    if volume < 1.0:
        return 0  # Observe
    elif volume < 1.5:
        return 1  # TMZ
    elif volume < 2.0:
        return 2  # RT
    else:
        return 3  # Combined

def sac_policy(state, actor):
    """SAC learned policy."""
    with torch.no_grad():
        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
        dist = actor(state_t)
        return dist.sample().item()

# ============================================================
# POLICY EVALUATION FUNCTION
# ============================================================

def evaluate_policy(env, policy_fn, n_episodes=1000, verbose=True):
    """Comprehensive policy evaluation with detailed metrics."""
    episode_lengths = []
    episode_rewards = []
    action_log = []
    toxicity_log = []
    volume_log = []

    if verbose:
        print(f"  Evaluating over {n_episodes} episodes...")

    for ep in range(n_episodes):
        state = env.reset()
        total_reward = 0
        episode_actions = []
        episode_toxicity = []
        episode_volumes = []

        for t in range(env.max_months):
            action = policy_fn(state)
            next_state, reward, done, info = env.step(action)

            episode_actions.append(action)
            episode_toxicity.append(info.get('toxicity', 0))
            episode_volumes.append(info.get('volume', 0))

            state = next_state
            total_reward += reward

            if done:
                break

        episode_lengths.append(t + 1)
        episode_rewards.append(total_reward)
        action_log.extend(episode_actions)
        toxicity_log.append(np.mean(episode_toxicity))
        volume_log.append(episode_volumes[-1] if episode_volumes else 0)

    return {
        "mean_survival": np.mean(episode_lengths),
        "std_survival": np.std(episode_lengths),
        "median_survival": np.median(episode_lengths),
        "min_survival": np.min(episode_lengths),
        "max_survival": np.max(episode_lengths),
        "mean_reward": np.mean(episode_rewards),
        "std_reward": np.std(episode_rewards),
        "survival_data": episode_lengths,
        "reward_data": episode_rewards,
        "actions": action_log,
        "mean_toxicity": np.mean(toxicity_log),
        "mean_final_volume": np.mean(volume_log),
    }

# ============================================================
# RUN BASELINE COMPARISONS 
# ============================================================

print("\n" + "="*70)
print("POLICY EVALUATION (Random, Heuristic, SAC)")
print("="*70)

# Assuming you have a trained SAC agent and environment
# agent, results, env_val = train_sac(seed=42)

results = {}

print("\n[1/3] Evaluating Random Policy...")
results["Random"] = evaluate_policy(env, random_policy, n_episodes=1000)

print("\n[2/3] Evaluating Heuristic Policy...")
results["Heuristic"] = evaluate_policy(env, heuristic_policy, n_episodes=1000)

print("\n[3/3] Evaluating SAC Policy...")
results["SAC"] = evaluate_policy(env, lambda s: sac_policy(s, agent.actor), n_episodes=1000)

# ============================================================
# COMPARISON TABLE
# ============================================================

print("\n" + "="*70)
print("POLICY PERFORMANCE SUMMARY")
print("="*70)

comparison_df = pd.DataFrame({
    policy: {
        "Mean Survival": f"{data['mean_survival']:.2f} Â± {data['std_survival']:.2f}",
        "Median Survival": f"{data['median_survival']:.1f}",
        "Range": f"[{data['min_survival']:.0f}, {data['max_survival']:.0f}]",
        "Mean Reward": f"{data['mean_reward']:.2f}",
    }
    for policy, data in results.items()
}).T

print("\n", comparison_df)

# ============================================================
# FIGURE 1: POLICY COMPARISON 
# ============================================================

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle('Policy Evaluation for Glioma Treatment Optimization',
             fontweight='bold', fontsize=15, y=1.02)

# Color palette (only 3 colors)
colors = {
    "Random": "#95A5A6",
    "Heuristic": "#E67E22",
    "SAC": "#27AE60"
}

policies = ["Random", "Heuristic", "SAC"]

# --- Panel A: Mean Survival Performance ---
ax1 = axes[0]

x_pos = np.arange(len(policies))
means = [results[p]["mean_survival"] for p in policies]
stds = [results[p]["std_survival"] for p in policies]

bars = ax1.bar(x_pos, means, yerr=stds, capsize=6,
               color=[colors[p] for p in policies],
               edgecolor='black', linewidth=1.2, alpha=0.85,
               error_kw={'linewidth': 2, 'elinewidth': 2, 'capthick': 2})

# Add value labels on bars
for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + std + 0.5,
             f'{mean:.1f}Â±{std:.1f}',
             ha='center', va='bottom', fontsize=10, fontweight='bold')

ax1.set_ylabel('Survival Time (months)', fontweight='bold', fontsize=12)
ax1.set_title('A. Mean Survival Performance', fontweight='bold', loc='left', fontsize=13, pad=10)
ax1.set_xticks(x_pos)
ax1.set_xticklabels(policies, fontweight='medium')
ax1.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.8)
ax1.set_axisbelow(True)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
ax1.set_ylim(0, max(means) + max(stds) + 3)

# --- Panel B: Treatment Strategy Distribution ---
ax2 = axes[1]

action_labels = ["Observe", "TMZ", "RT", "Combined"]
x_actions = np.arange(len(action_labels))
width = 0.25

for i, name in enumerate(policies):
    counts = Counter(results[name]["actions"])
    freqs = [counts.get(j, 0) / len(results[name]["actions"]) for j in range(4)]
    offset = (i - 1) * width
    ax2.bar(x_actions + offset, freqs, width, label=name,
            color=colors[name], alpha=0.85, edgecolor='black', linewidth=1)

ax2.set_ylabel('Action Probability', fontweight='bold', fontsize=12)
ax2.set_title('B. Treatment Strategy Distribution', fontweight='bold', loc='left', fontsize=13, pad=10)
ax2.set_xticks(x_actions)
ax2.set_xticklabels(action_labels, fontweight='medium')
ax2.legend(frameon=True, fancybox=True, shadow=True, edgecolor='black',
          loc='upper right', ncol=1)
ax2.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.8)
ax2.set_axisbelow(True)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)


ax3 = axes[2]

survival_data = [results[p]["survival_data"] for p in policies]
positions = [1, 2, 3]

parts = ax3.violinplot(survival_data, positions=positions, widths=0.6,
                        showmeans=True, showmedians=True)

# Color the violin plots
for i, pc in enumerate(parts['bodies']):
    pc.set_facecolor(colors[policies[i]])
    pc.set_alpha(0.7)
    pc.set_edgecolor('black')
    pc.set_linewidth(1.5)

# Style the elements
parts['cmeans'].set_color('black')
parts['cmeans'].set_linewidth(2)
parts['cmedians'].set_color('red')
parts['cmedians'].set_linewidth(2)
parts['cbars'].set_color('black')
parts['cmaxes'].set_color('black')
parts['cmins'].set_color('black')

ax3.set_ylabel('Survival Time (months)', fontweight='bold', fontsize=12)
ax3.set_title('C. Survival Distribution', fontweight='bold', loc='left', fontsize=13, pad=10)
ax3.set_xticks(positions)
ax3.set_xticklabels(policies, fontweight='medium')
ax3.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.8)
ax3.set_axisbelow(True)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)

# Add legend for violin plot
from matplotlib.lines import Line2D
legend_elements = [
    Line2D([0], [0], color='black', linewidth=2, label='Mean'),
    Line2D([0], [0], color='red', linewidth=2, label='Median')
]
ax3.legend(handles=legend_elements, loc='upper left', frameon=True,
          fancybox=True, shadow=True, edgecolor='black')

plt.tight_layout()
plt.savefig('policy_comparison_simple.png', dpi=300, bbox_inches='tight')
plt.savefig('policy_comparison_simple.pdf', bbox_inches='tight')
plt.show()

print("\nFigure 1 saved: policy_comparison_simple.png/.pdf")

# ============================================================
# STATISTICAL SIGNIFICANCE TESTING
# ============================================================

print("\n" + "="*70)
print("STATISTICAL SIGNIFICANCE TESTS")
print("="*70)

comparison_pairs = [
    ("SAC", "Random"),
    ("SAC", "Heuristic"),
]

for policy1, policy2 in comparison_pairs:
    statistic, p_value = stats.mannwhitneyu(
        results[policy1]["survival_data"],
        results[policy2]["survival_data"],
        alternative='two-sided'
    )

    sig_level = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
    mean_diff = results[policy1]["mean_survival"] - results[policy2]["mean_survival"]

    print(f"\n{policy1} vs {policy2}:")
    print(f"  Mean difference: {mean_diff:.2f} months")
    print(f"  p-value: {p_value:.4f} {sig_level}")

# ============================================================
# REWARD ABLATION STUDY
# ============================================================

print("\n" + "="*70)
print("ðŸ§ª REWARD ABLATION STUDY")
print("="*70)

class AblationEnv(GliomaTwinEnv):
    """Environment with simplified reward (no penalties)."""
    def step(self, action):
        self.month += 1
        self.state[4] *= self.growth

        if action == 1:
            self.state[4] *= self.chemo
            self.toxicity += 0.2
        elif action == 2:
            self.state[4] *= self.radio
            self.toxicity += 0.3
        elif action == 3:
            self.state[4] *= min(self.chemo, self.radio)
            self.toxicity += 0.5

        volume_delta = self.state[4] - self.initial_pca[4]
        self.state[7] += 0.05 * volume_delta
        self.state[-1] = float(self.month) / self.max_months

        # ABLATED REWARD: No penalties, survival only
        reward = 1.0
        done = False

        if self.state[7] > 3.0 or self.toxicity > 3.0:
            reward = -10.0
            done = True
        elif self.month >= self.max_months:
            done = True

        return self.state.astype(np.float32), reward, done, {
            'month': self.month, 'toxicity': self.toxicity, 'volume': self.state[4]
        }

# Create ablation environment
df_ablation = env.df # Use the df from the env returned by train_sac
scaler_ablation = env.scaler # Use the scaler from the env returned by train_sac
env_ablation = AblationEnv(df_ablation, scaler_ablation)

print("\nEvaluating SAC policy with ablated reward...")
ablation_results = evaluate_policy(
    env_ablation,
    lambda s: sac_policy(s, agent.actor),
    n_episodes=1000
)

# ============================================================
# FIGURE 2: ABLATION STUDY
# ============================================================

fig, axes = plt.subplots(1, 2, figsize=(12, 5))
fig.suptitle('Reward Design Ablation Study', fontweight='bold', fontsize=15, y=1.02)

ablation_labels = ["Full Reward\n(Toxicity Penalty)", "Survival-Only\n(Ablated)"]
ablation_colors = ["#27AE60", "#E74C3C"]

# --- Panel A: Survival Comparison ---
ax1 = axes[0]

means_abl = [results["SAC"]["mean_survival"], ablation_results["mean_survival"]]
stds_abl = [results["SAC"]["std_survival"], ablation_results["std_survival"]]

bars = ax1.bar(ablation_labels, means_abl, yerr=stds_abl, capsize=8,
               color=ablation_colors, edgecolor='black', linewidth=1.5, alpha=0.85,
               error_kw={'linewidth': 2.5, 'elinewidth': 2.5, 'capthick': 2.5})

# Add value labels
for bar, mean, std in zip(bars, means_abl, stds_abl):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + std + 0.4,
             f'{mean:.1f}Â±{std:.1f}',
             ha='center', va='bottom', fontsize=11, fontweight='bold')

# Calculate and show percentage difference
pct_diff = ((means_abl[0] - means_abl[1]) / means_abl[1]) * 100
mid_height = (means_abl[0] + means_abl[1]) / 2

ax1.annotate('', xy=(0, means_abl[0]), xytext=(1, means_abl[1]),
            arrowprops=dict(arrowstyle='<->', lw=2, color='#34495E'))
ax1.text(0.5, mid_height, f'{abs(pct_diff):.1f}% difference',
        ha='center', fontsize=10, fontweight='bold', color='#2C3E50',
        bbox=dict(boxstyle='round,pad=0.5', facecolor='white',
                  edgecolor='#34495E', linewidth=1.5))

ax1.set_ylabel('Mean Survival (months)', fontweight='bold', fontsize=12)
ax1.set_title('A. Impact of Reward Design on Survival', fontweight='bold', loc='left', fontsize=13, pad=10)
ax1.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.8)
ax1.set_axisbelow(True)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
ax1.set_ylim(0, max(means_abl) + max(stds_abl) + 2)

# --- Panel B: Action Distribution Comparison ---
ax2 = axes[1]

action_labels_short = ["Observe", "TMZ", "RT", "Combined"]
x_actions = np.arange(len(action_labels_short))
width = 0.35

counts_full = Counter(results["SAC"]["actions"])
freqs_full = [counts_full.get(j, 0) / len(results["SAC"]["actions"]) for j in range(4)]

counts_abl = Counter(ablation_results["actions"])
freqs_abl = [counts_abl.get(j, 0) / len(ablation_results["actions"]) for j in range(4)]

bars1 = ax2.bar(x_actions - width/2, freqs_full, width,
                label='Full Reward', color=ablation_colors[0],
                alpha=0.85, edgecolor='black', linewidth=1.2)
bars2 = ax2.bar(x_actions + width/2, freqs_abl, width,
                label='Survival-Only', color=ablation_colors[1],
                alpha=0.85, edgecolor='black', linewidth=1.2)

ax2.set_ylabel('Action Probability', fontweight='bold', fontsize=12)
ax2.set_title('B. Treatment Strategy Comparison', fontweight='bold', loc='left', fontsize=13, pad=10)
ax2.set_xticks(x_actions)
ax2.set_xticklabels(action_labels_short, fontweight='medium')
ax2.legend(frameon=True, fancybox=True, shadow=True, edgecolor='black',
          loc='upper right', ncol=1)
ax2.grid(axis='y', alpha=0.3, linestyle='--', linewidth=0.8)
ax2.set_axisbelow(True)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig('ablation_study_simple.png', dpi=300, bbox_inches='tight')
plt.savefig('ablation_study_simple.pdf', bbox_inches='tight')
plt.show()

print("\n Figure 2 saved: ablation_study_simple.png/.pdf")

# Statistical test for ablation
_, p_ablation = stats.mannwhitneyu(
    results["SAC"]["survival_data"],
    ablation_results["survival_data"]
)

print("\nAblation Study Results:")
print(f"  Full Reward:   {means_abl[0]:.2f} Â± {stds_abl[0]:.2f} months")
print(f"  Survival-Only: {means_abl[1]:.2f} Â± {stds_abl[1]:.2f} months")
print(f"  Difference:    {abs(means_abl[0] - means_abl[1]):.2f} months ({abs(pct_diff):.1f}%)")
print(f"  p-value:       {p_ablation:.4f} {'***' if p_ablation < 0.001 else '**' if p_ablation < 0.01 else '*' if p_ablation < 0.05 else 'ns'}")

# ============================================================
# FINAL SUMMARY
# ============================================================

print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

print(f"\n Best Policy: SAC")
print(f"   Mean Survival: {results['SAC']['mean_survival']:.2f} Â± {results['SAC']['std_survival']:.2f} months")

print(f"\nðŸ“ˆ SAC Improvements over Baselines:")
for policy in ["Random", "Heuristic"]:
    improvement = ((results["SAC"]["mean_survival"] - results[policy]["mean_survival"]) /
                  results[policy]["mean_survival"] * 100)
    print(f"   vs {policy:10s}: +{improvement:6.2f}%")

print("\n" + "="*70)
print(" EVALUATION COMPLETE")
print("="*70)
