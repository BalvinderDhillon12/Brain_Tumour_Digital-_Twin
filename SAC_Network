import gym
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
from tqdm import tqdm
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from typing import Tuple, Dict, List, Optional # Import Optional
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter1d

# font
plt.rcParams.update({
    'font.family': 'serif',
    'font.size': 11,
    'axes.labelsize': 12,
    'axes.titlesize': 13,
    'xtick.labelsize': 11,
    'ytick.labelsize': 11,
    'legend.fontsize': 10,
    'figure.titlesize': 14,
    'figure.dpi': 150,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'savefig.pad_inches': 0.15,
    'axes.linewidth': 1.2,
    'grid.linewidth': 0.6,
    'lines.linewidth': 2.0,
    'axes.grid': True,
    'grid.alpha': 0.25,
    'grid.linestyle': '--',
    'axes.axisbelow': True,
})

SEED_COLORS = ['#0173B2', '#DE8F05', '#029E73', '#CC78BC', '#CA9161']

#config

class Config:
    MAX_MONTHS = 24
    DATA_PATH = "/content/drive/MyDrive/hackathon_output/PCA_data/RADIOMICS_PCA_DATA.csv"
    EPISODES = 2000
    BATCH_SIZE = 64
    BUFFER_SIZE = 100000
    HIDDEN_DIM = 128
    NUM_LAYERS = 2
    DROPOUT = 0.0
    LR_ACTOR = 1e-4
    LR_CRITIC = 1e-4
    LR_ALPHA = 1e-4
    GAMMA = 0.99
    TAU = 0.01
    GRAD_CLIP = 0.5
    INIT_ALPHA = 0.2
    MAX_ALPHA = 1.0
    TARGET_ENTROPY_SCALE = 0.98
    EVAL_FREQUENCY = 100
    EVAL_EPISODES = 50
    SEEDS = [42, 123, 456]
    SAVE_DIR = Path("./outputs")
    LOG_FREQUENCY = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#env

class GliomaTwinEnv(gym.Env):
    GROWTH_RANGE = (1.02, 1.04)
    CHEMO_EFFICACY = (0.90, 0.95)
    RADIO_EFFICACY = (0.85, 0.92)
    TOXICITY_TOLERANCE = (0.9, 1.1)
    VOLUME_PENALTY = 0.1
    TOXICITY_PENALTY = 0.5
    TERMINAL_PENALTY = -10.0

    def __init__(self, data_df: pd.DataFrame, scaler: StandardScaler,
                 max_months: int = Config.MAX_MONTHS):
        super().__init__()
        self.df = data_df
        self.scaler = scaler
        self.max_months = max_months
        self.pc_cols = [col for col in self.df.columns if col.startswith('PC_')]
        self.action_space = gym.spaces.Discrete(4)
        self.state_dim = len(self.pc_cols) + 1
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.state_dim,), dtype=np.float32
        )

    def reset(self, patient_idx: Optional[int] = None) -> np.ndarray: # Added patient_idx parameter
        if patient_idx is not None:
            # Select a specific patient by index
            if patient_idx >= len(self.df):
                raise ValueError(f"patient_idx {patient_idx} out of bounds for df of size {len(self.df)}")
            row = self.df.iloc[patient_idx]
        else:
            # If no specific patient index, pick one at random
            row = self.df.sample(1).iloc[0]

        pca_features = row[self.pc_cols].values.astype(np.float32)
        self.initial_pca = self.scaler.transform(pca_features.reshape(1, -1)).flatten()

        self.growth = np.random.uniform(*self.GROWTH_RANGE)
        self.chemo = np.random.uniform(*self.CHEMO_EFFICACY)
        self.radio = np.random.uniform(*self.RADIO_EFFICACY)
        self.tolerance = np.random.uniform(*self.TOXICITY_TOLERANCE)

        self.toxicity = 0.0
        self.month = 0
        self.state = np.append(self.initial_pca.copy(), [0.0])
        return self.state.astype(np.float32)

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        self.month += 1
        self.state[4] *= self.growth

        if action == 1:
            self.state[4] *= self.chemo
            self.toxicity += 0.2
        elif action == 2:
            self.state[4] *= self.radio
            self.toxicity += 0.3
        elif action == 3:
            self.state[4] *= min(self.chemo, self.radio)
            self.toxicity += 0.5

        volume_delta = self.state[4] - self.initial_pca[4]
        self.state[7] += 0.05 * volume_delta
        self.state[-1] = float(self.month) / self.max_months

        reward = 1.0 - self.VOLUME_PENALTY * max(0, self.state[4] - 1.0)
        reward -= self.TOXICITY_PENALTY * (self.toxicity / self.tolerance)

        done = False
        if self.state[7] > 3.0 or self.toxicity > 3.0:
            reward = self.TERMINAL_PENALTY
            done = True
        elif self.month >= self.max_months:
            done = True

        return self.state.astype(np.float32), reward, done, {
            'month': self.month, 'toxicity': self.toxicity, 'volume': self.state[4]
        }

#sac network

class Actor(nn.Module):
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )

    def forward(self, state: torch.Tensor):
        logits = self.net(state)
        return torch.distributions.Categorical(logits=logits)

class Critic(nn.Module):
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        self.action_dim = action_dim
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, state: torch.Tensor, action: torch.Tensor):
        action_onehot = nn.functional.one_hot(action, self.action_dim).float()
        x = torch.cat([state, action_onehot], dim=1)
        return self.net(x)

#replay buffer

class ReplayBuffer:
    def __init__(self, size: int = Config.BUFFER_SIZE):
        self.buffer = deque(maxlen=size)

    def add(self, s, a, r, s2, d):
        self.buffer.append((s, a, r, s2, d))

    def sample(self, batch_size: int, device):
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        s, a, r, s2, d = zip(*[self.buffer[i] for i in indices])
        return (
            torch.FloatTensor(np.array(s)).to(device),
            torch.LongTensor(a).to(device),
            torch.FloatTensor(r).to(device),
            torch.FloatTensor(np.array(s2)).to(device),
            torch.FloatTensor(d).to(device)
        )

    def __len__(self):
        return len(self.buffer)

#sac agent

class SACAgent:
    def __init__(self, state_dim: int, action_dim: int):
        self.action_dim = action_dim

        self.actor = Actor(state_dim, action_dim).to(device)
        self.critic1 = Critic(state_dim, action_dim).to(device)
        self.critic2 = Critic(state_dim, action_dim).to(device)
        self.critic1_target = Critic(state_dim, action_dim).to(device)
        self.critic2_target = Critic(state_dim, action_dim).to(device)
        self.critic1_target.load_state_dict(self.critic1.state_dict())
        self.critic2_target.load_state_dict(self.critic2.state_dict())

        self.actor_opt = optim.Adam(self.actor.parameters(), lr=Config.LR_ACTOR)
        self.critic1_opt = optim.Adam(self.critic1.parameters(), lr=Config.LR_CRITIC)
        self.critic2_opt = optim.Adam(self.critic2.parameters(), lr=Config.LR_CRITIC)

        self.target_entropy = -np.log(1.0 / action_dim) * Config.TARGET_ENTROPY_SCALE
        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=Config.LR_ALPHA)

        self.buffer = ReplayBuffer()

    @property
    def alpha(self):
        return min(self.log_alpha.exp().item(), Config.MAX_ALPHA)

    def select_action(self, state, deterministic=False):
        with torch.no_grad():
            state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
            dist = self.actor(state_t)
            action = dist.probs.argmax().item() if deterministic else dist.sample().item()
        return action

    def update(self):
        if len(self.buffer) < Config.BATCH_SIZE:
            return {}

        s, a, r, s2, d = self.buffer.sample(Config.BATCH_SIZE, device)

        with torch.no_grad():
            next_dist = self.actor(s2)
            next_a = next_dist.sample()
            next_logp = next_dist.log_prob(next_a)
            next_q1 = self.critic1_target(s2, next_a)
            next_q2 = self.critic2_target(s2, next_a)
            next_q = torch.min(next_q1, next_q2)
            target_q = r.unsqueeze(1) + Config.GAMMA * (1 - d.unsqueeze(1)) * (
                next_q - self.alpha * next_logp.unsqueeze(1)
            )

        q1 = self.critic1(s, a)
        critic1_loss = nn.MSELoss()(q1, target_q)
        self.critic1_opt.zero_grad()
        critic1_loss.backward()
        nn.utils.clip_grad_norm_(self.critic1.parameters(), Config.GRAD_CLIP)
        self.critic1_opt.step()

        q2 = self.critic2(s, a)
        critic2_loss = nn.MSELoss()(q2, target_q)
        self.critic2_opt.zero_grad()
        critic2_loss.backward()
        nn.utils.clip_grad_norm_(self.critic2.parameters(), Config.GRAD_CLIP)
        self.critic2_opt.step()

        dist = self.actor(s)
        a_sample = dist.sample()
        logp = dist.log_prob(a_sample)
        q1_pi = self.critic1(s, a_sample)
        q2_pi = self.critic2(s, a_sample)
        q_pi = torch.min(q1_pi, q2_pi)
        actor_loss = (self.alpha * logp.unsqueeze(1) - q_pi).mean()

        self.actor_opt.zero_grad()
        actor_loss.backward()
        nn.utils.clip_grad_norm_(self.actor.parameters(), Config.GRAD_CLIP)
        self.actor_opt.step()

        alpha_loss = -(self.log_alpha * (logp + self.target_entropy).detach()).mean()
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()

        with torch.no_grad():
            self.log_alpha.clamp_(max=np.log(Config.MAX_ALPHA))

        for p, p_targ in zip(self.critic1.parameters(), self.critic1_target.parameters()):
            p_targ.data.copy_(Config.TAU * p.data + (1 - Config.TAU) * p_targ.data)
        for p, p_targ in zip(self.critic2.parameters(), self.critic2_target.parameters()):
            p_targ.data.copy_(Config.TAU * p.data + (1 - Config.TAU) * p_targ.data)

        return {
            'critic1_loss': critic1_loss.item(),
            'critic2_loss': critic2_loss.item(),
            'actor_loss': actor_loss.item(),
            'alpha': self.alpha,
            'alpha_loss': alpha_loss.item()
        }

#training

def train_sac(seed: int = 42):
    np.random.seed(seed)
    torch.manual_seed(seed)

    df = pd.read_csv(Config.DATA_PATH)
    pc_cols = [col for col in df.columns if col.startswith('PC_')]

    train_df = df.sample(frac=0.8, random_state=seed)
    val_df = df.drop(train_df.index)

    scaler = StandardScaler()
    scaler.fit(train_df[pc_cols])

    env_train = GliomaTwinEnv(train_df, scaler)
    env_val = GliomaTwinEnv(val_df, scaler)

    agent = SACAgent(env_train.state_dim, env_train.action_space.n)

    train_rewards = []
    train_lengths = []
    val_metrics = []
    losses = []

    print(f"\n{'='*60}")
    print(f"Training SAC Agent (Seed: {seed})")
    print(f"{'='*60}")

    pbar = tqdm(range(Config.EPISODES), desc=f"Seed {seed}")

    for episode in pbar:
        state = env_train.reset()
        episode_reward = 0

        for t in range(Config.MAX_MONTHS):
            action = agent.select_action(state, deterministic=False)
            next_state, reward, done, _ = env_train.step(action)

            agent.buffer.add(state, action, reward, next_state, done)

            if len(agent.buffer) >= Config.BATCH_SIZE:
                loss_dict = agent.update()
                if loss_dict:
                    losses.append(loss_dict)

            state = next_state
            episode_reward += reward

            if done:
                break

        train_rewards.append(episode_reward)
        train_lengths.append(t + 1)

        if episode % Config.LOG_FREQUENCY == 0:
            recent_survival = np.mean(train_lengths[-Config.LOG_FREQUENCY:])
            pbar.set_postfix({
                'survival': f'{recent_survival:.1f}',
                'alpha': f'{agent.alpha:.3f}'
            })

        if (episode + 1) % Config.EVAL_FREQUENCY == 0:
            val_survivals = []
            for _ in range(Config.EVAL_EPISODES):
                state = env_val.reset()
                for t in range(env_val.max_months):
                    action = agent.select_action(state, deterministic=True)
                    state, reward, done, _ = env_val.step(action)
                    if done:
                        break
                val_survivals.append(t + 1)

            val_metrics.append({
                'episode': episode + 1,
                'mean_survival': np.mean(val_survivals),
                'std_survival': np.std(val_survivals)
            })

    print(f"\nTr aining completed")
    print(f"   Mean Survival: {np.mean(train_lengths):.2f} \u00b1 {np.std(train_lengths):.2f} months")

    return agent, {
        'train_rewards': train_rewards,
        'train_lengths': train_lengths,
        'val_metrics': val_metrics,
        'losses': losses,
        'seed': seed
    }, env_val

#plotting

def smooth_curve(data: np.ndarray, sigma: float = 2.0) -> np.ndarray:
    if len(data) < 10:
        return data
    return gaussian_filter1d(data, sigma=sigma)

def plot_publication_figure(all_results: List[Dict], save_dir: Path):
    fig = plt.figure(figsize=(14, 10))
    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)

    fig.suptitle('Soft Actor-Critic Training Dynamics for Glioma Treatment Optimization',
                 fontweight='bold', fontsize=15, y=0.98)

    # panel A: Training Survival Curves
    ax1 = fig.add_subplot(gs[0, 0])
    for idx, result in enumerate(all_results):
        seed = result['seed']
        survival = np.array(result['train_lengths'])
        smoothed = smooth_curve(survival, sigma=15.0)
        episodes = np.arange(len(smoothed))
        ax1.plot(episodes, smoothed, label=f'Seed {seed}',
                color=SEED_COLORS[idx % len(SEED_COLORS)], alpha=0.85, linewidth=2.2)

    ax1.set_xlabel('Training Episode', fontweight='bold', fontsize=12)
    ax1.set_ylabel('Survival Duration (months)', fontweight='bold', fontsize=12)
    ax1.set_title('A. Training Survival Curves', fontweight='bold', loc='left', fontsize=13, pad=10)
    ax1.legend(frameon=True, fancybox=True, shadow=True, loc='best', framealpha=0.95)
    ax1.set_xlim(0, len(all_results[0]['train_lengths']))
    ax1.set_ylim(bottom=0)
    ax1.spines['top'].set_visible(False)
    ax1.spines['right'].set_visible(False)

    # panel B: Training Rewards
    ax2 = fig.add_subplot(gs[0, 1])
    for idx, result in enumerate(all_results):
        seed = result['seed']
        rewards = np.array(result['train_rewards'])
        smoothed = smooth_curve(rewards, sigma=15.0)
        episodes = np.arange(len(smoothed))
        ax2.plot(episodes, smoothed, label=f'Seed {seed}',
                color=SEED_COLORS[idx % len(SEED_COLORS)], alpha=0.85, linewidth=2.2)

    ax2.set_xlabel('Training Episode', fontweight='bold', fontsize=12)
    ax2.set_ylabel('Episode Reward', fontweight='bold', fontsize=12)
    ax2.set_title('B. Training Rewards', fontweight='bold', loc='left', fontsize=13, pad=10)
    ax2.legend(frameon=True, fancybox=True, shadow=True, loc='best', framealpha=0.95)
    ax2.set_xlim(0, len(all_results[0]['train_rewards']))
    ax2.axhline(y=0, color='black', linestyle='--', linewidth=1.0, alpha=0.3)
    ax2.spines['top'].set_visible(False)
    ax2.spines['right'].set_visible(False)

    # panel C: Validation Performance
    ax3 = fig.add_subplot(gs[1, 0])
    for idx, result in enumerate(all_results):
        seed = result['seed']
        val_metrics = result['val_metrics']
        if val_metrics:
            episodes = [m['episode'] for m in val_metrics]
            means = [m['mean_survival'] for m in val_metrics]
            stds = [m['std_survival'] for m in val_metrics]
            color = SEED_COLORS[idx % len(SEED_COLORS)]
            ax3.plot(episodes, means, label=f'Seed {seed}', color=color,
                    linewidth=2.5, marker='o', markersize=5, alpha=0.9)
            ax3.fill_between(episodes, np.array(means) - np.array(stds),
                           np.array(means) + np.array(stds), color=color, alpha=0.15)

    ax3.set_xlabel('Training Episode', fontweight='bold', fontsize=12)
    ax3.set_ylabel('Validation Survival (months)', fontweight='bold', fontsize=12)
    ax3.set_title('C. Validation Performance', fontweight='bold', loc='left', fontsize=13, pad=10)
    ax3.legend(frameon=True, fancybox=True, shadow=True, loc='best', framealpha=0.95)
    ax3.set_ylim(bottom=0)
    ax3.spines['top'].set_visible(False)
    ax3.spines['right'].set_visible(False)

    # panel D: Entropy Temperature
    ax4 = fig.add_subplot(gs[1, 1])
    for idx, result in enumerate(all_results):
        seed = result['seed']
        losses = result['losses']
        if losses:
            alphas = [loss['alpha'] for loss in losses]
            steps = np.arange(len(alphas))
            if len(alphas) > 100:
                smoothed_alpha = smooth_curve(np.array(alphas), sigma=50.0)
            else:
                smoothed_alpha = alphas
            ax4.plot(steps, smoothed_alpha, label=f'Seed {seed}',
                    color=SEED_COLORS[idx % len(SEED_COLORS)], linewidth=2.2, alpha=0.85)

    ax4.set_xlabel('Update Step', fontweight='bold', fontsize=12)
    ax4.set_ylabel('Temperature (\u03b1)', fontweight='bold', fontsize=12)
    ax4.set_title('D. Entropy Temperature Tuning', fontweight='bold', loc='left', fontsize=13, pad=10)
    ax4.legend(frameon=True, fancybox=True, shadow=True, loc='best', framealpha=0.95)
    ax4.set_ylim(bottom=0)
    ax4.spines['top'].set_visible(False)
    ax4.spines['right'].set_visible(False)

    save_dir.mkdir(parents=True, exist_ok=True)
    plt.savefig(save_dir / 'sac_training_results.png', dpi=300, bbox_inches='tight')
    plt.savefig(save_dir / 'sac_training_results.pdf', dpi=300, bbox_inches='tight')
    print(f"\nðŸ“Š Figures saved to {save_dir}")
    plt.show()
    plt.close()

#main

if __name__ == "__main__":
    Config.SAVE_DIR.mkdir(parents=True, exist_ok=True)

    print("\ud83d\ude80 Starting SAC Training with Publication-Ready Visualizations")
    print(f"Configuration: {Config.EPISODES} episodes, {len(Config.SEEDS)} seeds")

    all_results = []
    all_agents = []

    for seed in Config.SEEDS:
        agent, results, env = train_sac(seed=seed)
        all_results.append(results)
        all_agents.append(agent)

    plot_publication_figure(all_results, Config.SAVE_DIR)

    print("\n Training complete!")
    print(f"\nFinal Performance Summary:")
    print("="*60)
    for result in all_results:
        seed = result['seed']
        final_survival = np.mean(result['train_lengths'][-100:])
        print(f"Seed {seed}: {final_survival:.2f} \u00b1 {np.std(result['train_lengths'][-100:]):.2f} months")

    overall_mean = np.mean([np.mean(r['train_lengths'][-100:]) for r in all_results])
    overall_std = np.std([np.mean(r['train_lengths'][-100:]) for r in all_results])
    print(f"\nOverall: {overall_mean:.2f} \u00b1 {overall_std:.2f} months")
    print("="*60)
